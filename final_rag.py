# -*- coding: utf-8 -*-
"""final-rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jht5HNOytPiaLtZbnh04acth92FGhFmx
"""

!pip install langchain langchain_community faiss-cpu
!pip install pypdf

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain.prompts import PromptTemplate

from langchain.chains import RetrievalQA

import os
directory="/content"
pdf_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.lower().endswith('.pdf')]

## Read the ppdfs from the folder

loader=PyPDFDirectoryLoader("/content")
documents=loader.load()
text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)
final_documents=text_splitter.split_documents(documents)
final_documents[1]

type(final_documents[0])

final_documents

len(final_documents)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install sentence_transformers

## Embedding Using Huggingface
huggingface_embeddings=HuggingFaceBgeEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",      #sentence-transformers/all-MiniLM-l6-v2
    model_kwargs={'device':'cpu'},
    encode_kwargs={'normalize_embeddings':True}

)

import  numpy as np
print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))
print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)

## VectorStore Creation
vectorstore=FAISS.from_documents(final_documents,huggingface_embeddings)

## Query using Similarity Search
query="What is conventional current?"
relevant_docments=vectorstore.similarity_search(query)

print(relevant_docments[0].page_content)

retriever=vectorstore.as_retriever(search_type="similarity",search_kwargs={"k":3})
print(retriever)

import os
os.environ['HUGGINGFACEHUB_API_TOKEN']="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"

from langchain_community.llms import HuggingFaceHub

hf=HuggingFaceHub(
    repo_id="mistralai/Mistral-7B-v0.1",
    model_kwargs={"temperature":0.1,"max_length":500}

)
query="What is conventional current?"
hf.invoke(query)

#Hugging Face models can be run locally through the HuggingFacePipeline class.
"""from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline

hf = HuggingFacePipeline.from_model_id(
    model_id="mistralai/Mistral-7B-v0.1",
    task="text-generation",
    pipeline_kwargs={"temperature": 0, "max_new_tokens": 300}
)

llm = hf
llm.invoke(query)"""

prompt_template="""
Use the following piece of context to answer the question asked.
Please try to provide the answer only based on the context

{context}
Question:{question}

Helpful Answers:
 """

prompt=PromptTemplate(template=prompt_template,input_variables=["context","question"])

retrievalQA=RetrievalQA.from_chain_type(
    llm=hf,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=False,
    chain_type_kwargs={"prompt":prompt}
)

query="""What is conventional current?"""

# # Call the QA chain with our query.
# result = retrievalQA.invoke({"query": query})

# print(result['result'])

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)


question_answer_chain = create_stuff_documents_chain(hf, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

results = rag_chain.invoke({"input": "What is conventional current?"})

print(results["answer"],results["context"][0].metadata)

